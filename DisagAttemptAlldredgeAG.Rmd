---
title: "Diagnosed Dissaggregation Try 2"
output: html_notebook
---

Sequal to TryToDiagnoseDisagg.Rmd
Which didn't work the first time and kind of became a mess.
And FluxInterceptSillyness.Rmd

24 March 2019

Start with core parts of the FluxInterceptSillyness solution

Then lets run dissaggregation on the flux profile.

# Making a synthetic data set.

Data to make binned_EachSize_ald.csv come from Normalize_UVP_Flux.Rmd
Which in turn pulls data generated by UVP2017.Rmd

```{r, message= FALSE}
options(readr.default_locale=readr::locale(tz="Mexico/General"))
source("UVP_2017_library.R")
#dataP2 <- bring_in_p2()
#twin01 <- make_twin_df_list(dataP2)
#ES01 <- twin01[[1]] %>% filter(depth <= 1000, profile == "stn_043")
#DS01 <- twin01[[2]] %>% filter(depth <= 1000, profile == "stn_043")
# # Modification, merge all stations and see what happens.
# ES01 <- twin01[[1]] #%>% filter(depth <= 1000)
# DS01 <- twin01[[2]] #%>% filter(depth <= 1000)

ES01 <- read_csv("dataOut/binned_EachSize.csv") %>% filter(depth <= 1000)
DS01 <- read_csv("dataOut/binned_DepthSummary.csv") %>% filter(depth <= 1000)
twinS <- list(ES01, DS01)
```

```{r}
SimpleBins <- seq(from = 0, to = 1100, by = 100)
``` 

```{r}
#debug(bin_depths)
binnedS <- bin_depths(twinS, bins = SimpleBins) %>% calc_psd
```

```{r}
ESS <- binnedS[[1]]
DSS <- binnedS[[2]]
```



# Different direction
Units of nparticles

```{r}
# mySpec <- c(3.32405678509316, 1.85877856561554, 1.03359305917877, 0.564880885240349, 
# 0.309079523670959, 0.165424066706246, 0.0925270232480783, 0.0502491579183327, 
# 0.0275765808581977, 0.0147327962322043, 0.00847342228148116, 
# 0.00457079526918403, 0.00242657514481869, 0.00133840064588931, 
# 0.00073768231262027, 0.000406688319239427, 0.00021922892226164, 
# 0.000120831754575073, 6.6223357185171e-05, 3.58979192483917e-05, 
# 2.00799713729794e-05, 1.09115166969355e-05, 5.82645914172791e-06, 
# 3.28908275460212e-06, 1.57702715137e-06)

lb_vec <- c(0.102, 0.128, 0.161, 0.203, 0.256, 0.323, 0.406, 0.512, 0.645, 
0.813, 1.02, 1.29, 1.63, 2.05, 2.58, 3.25, 4.1, 5.16, 6.5, 8.19, 
10.3, 13, 16.4, 20.6, 26)

binsize_vec <- c(0.026, 0.033, 0.042, 0.053, 0.067, 0.083, 0.106, 0.133, 0.168, 
0.207, 0.27, 0.34, 0.42, 0.53, 0.67, 0.85, 1.06, 1.34, 1.69, 
2.11, 2.7, 3.4, 4.2, 5.4, 6)
```

```{r}
# C_f = 4
#ag = 0.26 # as in rest of universe
C_f = 10.51 # for now
alpha = .52
gamma = .26
ag = alpha + gamma
# mySpec
# myDf <- tibble(lb_vec, mySpec, binsize_vec)
```


```{r}
make_spectrum <- function(icp, psd, bins = lb_vec, binsizes = binsize_vec){
  C_n = exp(icp)
  nnp = (C_n * bins ^ psd) * (binsizes)
  nnp
}

testSpectrum <- make_spectrum( -2, -3.5)
testSpectrum
```

# Post for Stack overflow, at least in thory

```{r}
library(tidyverse)
library(cowplot)
```


```{r}
particle_data <- ESS %>% select(-nparticles, -n_nparticles, -time, -binsize)
```


I have a dataset in which I have measured particles of different sizes at different depths. Each depth has several size bins associated with it
lb: lower bound of particle size (mm)
ub: upper bound of particle size (mm)
vol: amount of water sampled per depth (l)
TotalParticles: The number of particles seen in that volume (#)
depth: The depth we are sampling (m)

We process the data slightly
```{r}
particle_data_processed <- particle_data %>% 
  mutate(binsize = ub-lb, # size of particle bins
         nparticles = TotalParticles/vol, # particles normalized to volume (#/L)
         n_nparticles = nparticles/binsize, # particles normalized to volume and bin size (#/L/mm)
  )
particle_data_processed
```

I model the relationship between particles and depth as a power law function. That is the log of the particle size is linearly related to the log of the size and volume normalized particle numbers. To account for zeros, I use a poisson glm
```{r}
myGlm <- function(df){
  glm(TotalParticles ~ log(lb), offset = log(vol * binsize), family = poisson, data = df)
}
particle_icp_psd <- particle_data_processed %>% nest(-depth, -profile) %>% 
  mutate(model = map(data, myGlm)) %>%
  mutate(tidied = map(model, tidy)) %>%
  unnest(tidied) %>%
  select(depth, profile, term, estimate) %>%
  spread(key = term, value = estimate) %>%
  rename(icp = `(Intercept)`, psd = `log(lb)`)
particle_icp_psd
```

One more thing. I also care about the particle flux. Flux is the sum of all of the particles times their sinking speed, times their mass. I relate flux to mass as follows
Flux{in a bin of diamter D} = C_f * D ^ ag
Cf and and ag are constants 
Cf = 4, ag = 0.23
And Total Flux = sum_D{Flux(D)}
Flux is calculated based on the particles normalized to volume

```{r}
#C_f = 4
#ag = 0.23
particle_flux <- particle_data_processed %>% 
  mutate(flux = (C_f * nparticles ^ ag)) %>%
  group_by(depth, profile) %>%
  summarize(Flux = sum(flux))
particle_flux
```



I combine the flux data with the PSD and intercept data
```{r}
particle_ipf <- left_join(particle_icp_psd, particle_flux, by = "depth")
particle_ipf
```

```{r}
pFlux <- ggplot(particle_ipf, aes(x = Flux, y = depth)) + scale_y_reverse() + geom_point()
pPSD <- ggplot(particle_ipf, aes(x = psd, y = depth)) + scale_y_reverse() + geom_point()
picp <- ggplot(particle_ipf, aes(x = icp, y = depth)) + scale_y_reverse() + geom_point()
plot_grid(pFlux, pPSD, picp, nrow = 1)
```

Now, for "reasons", I want to smooth these profiles out and interpolate some new spectra. First, I use gams to model the three profiles.

```{r}
gamPSD <- gam(psd ~ s(depth), data = particle_ipf)
gamicp <- gam(icp ~ s(depth), data = particle_ipf)
gamFlux <- gam(Flux ~ s(depth), family = "Gamma", data = particle_ipf)
```

```{r}
predData <- tibble(depth = seq(from = 0, to = 1000, by = 25))
predPSD <- predict(gamPSD, predData, type = "response")
predicp <- predict(gamicp, predData, type = "response")
predFlux <- predict(gamFlux, predData, type = "response")

predDf <- tibble(depth = predData$depth, psd = predPSD, icp = predicp, Flux = predFlux)
predDf
```

```{r}
pFlux <- ggplot(predDf, aes(x = Flux, y = depth)) + scale_y_reverse() + geom_point() + scale_x_continuous(limits = c(0, 200))
pPSD <- ggplot(predDf, aes(x = psd, y = depth)) + scale_y_reverse() + geom_point()
pInt <- ggplot(predDf, aes(x = icp, y = depth)) + scale_y_reverse() + geom_point() 
plot_grid(pFlux, pPSD, pInt, nrow = 1)
```

So next, I'm going to recreate spectra from psd and int. I will see if these recreated spectra approximate the observed "Flux" values.

Recall that psd and int relate to binsize and volume normalized particle numbers.

```{r}
lb_vec = particle_data %>% pull(lb) %>% unique # the particle sizes
binsize_vec = particle_data_processed %>% pull(binsize) %>% unique # the particle sizes

make_spectrum <- function(icp, psd, bins = lb_vec, binsizes = binsize_vec){
  C_n = exp(icp)
  nnp = (C_n * bins ^ psd) * (binsizes)
  nnp
}

make_spectrum <- function(icp, psd, bins = lb_vec, binsizes = binsize_vec){
  nnp = exp(log(bins) * psd + icp)
  np = nnp * binsizes
  tibble(lb = bins, np)
}


predDf <- predDf %>% mutate(spec = map2(icp, psd, make_spectrum))
```

```{r}
predDf[["spec"]][[5]] %>% mutate(flux = (C_f * np ^ag)) %>% summarize(Flux = sum(flux))
```


Now, I re-calculate flux from spec.

```{r}
sumflux <- function(df){
  df %>% pull(flux) %>% sum
}
predDf2 <- predDf %>% 
  mutate(spec = map(spec, 
                    . %>% mutate(flux = (C_f * np ^ ag))
                    )) %>%
  #mutate(Flux = map(spec, ~ . %>% summarise(Flux = sum(flux))))
  mutate(Flux = map_dbl(spec, sumflux))
```

```{r}
pPredFlux <- ggplot(data = predDf2, aes(y = depth, x = Flux)) + geom_point() + scale_y_reverse() + scale_x_continuous(limits = c(0, 200))
plot_grid(pFlux, pPredFlux)
```

# Unnest PredDf

```{r}
predDf3 <- predDf2 %>% unnest(spec)
```

# Disagg part

## Data
```{r}
synthetic_data <- predDf2 %>% 
  mutate(spec_only = map(spec, ~pull(., np)),
    prev_spec = lag(spec_only), 
         prev_Flux = lag(Flux),
         DF = prev_Flux - Flux,
         DFP = 1-DF/prev_Flux
  )
```

## Variabiles
```{r}

# mass of a 1mm particle
m1mm = 3.3 * 10^-6; #%g % Alldgedge 1998 % mass of 1mm particle
w1mm = 2; #% m/day # Alldredge and Gotschalk, methinks % sinking speed of 1mm particle
micron = 1e-6;
# fractle dimension
#C_f = 4 # Usual Way
#ag = 0.26

# So we can conform to usual expectations of alpha and gamma being positive
C_f = C_f # defined earlier
ag = ag # defined earlier

Cm = m1mm
Cw = w1mm
m_vec =  Cm * lb_vec ^ alpha;
w_vec = Cw * lb_vec ^ gamma;

test_abun_in <- synthetic_data %>% filter(depth == 25) %>% pull(spec) %>%.[[1]] %>% pull(np)

f_vec0 <- test_abun_in * m_vec * w_vec
F0 <- sum(f_vec0)

test_abun_out <- synthetic_data %>% filter(depth == 50) %>% pull(spec) %>% .[[1]] %>% pull(np)
f_vec1 <- test_abun_out * m_vec * w_vec
F1 <- sum(f_vec1)
DFP <- F1/F0
DFP

little_lb <- lb_vec[1] - (lb_vec[2] - lb_vec[1])/2 # size of the particle that the UVP can't see anymore. Eg, things actually shrink to this size but then they vanish from the UVP's view. Just leting it be like, the difference in size of the smallest two bins smaller than the smallest bin.
```

```{r}
remin_shuffle <- function(abun_in, DFpct, DeltaZ = 10, size = lb_vec, Cm = m1mm, Cw = w1mm, lbv = lb_vec, mv = m_vec, wv = w_vec,
                          alpha = 2.3, gamma = alpha - 1, llb = little_lb){
 rn = abun_in * lb_vec
 ran = abun_in * lb_vec ^ alpha
 srn = sum(rn)
 sran = sum(ran)
 
 omega = lb_vec[1] ^ (2 * alpha)  * abun_in[1]/(lb_vec[1] ^ alpha - llb ^ alpha)
 
 nmw = abun_in * mv * wv
 F1 = sum(nmw)
 DeltaF = (F1 * DFpct) - F1 # should be negative
 
 Cr = DeltaF/ (Cm*(1+gamma/alpha) * DeltaZ * (sran + omega));
 CrCw = Cr/Cw
 
 phi = CrCw * (1+gamma/alpha) * ran * 
   DeltaZ/
   (c(llb, lbv)[1:length(lbv)]^alpha - lbv ^ alpha) # added extra parentheses
 
 Delta_nj_out = phi/lbv ^ gamma
 Delta_nj_in = c(phi[2:length(phi)],0)/c(lbv[2:length(phi)],1) ^ gamma
 #Delta_nj_in = -c(phi[2:length(phi)],0)/lbv ^ gamma
 Delta_nj_net = Delta_nj_in - Delta_nj_out # positive because out is negative
   
  return(list(Cr = Cr, phi = phi, dnet = Delta_nj_net, din =  Delta_nj_in, dout = Delta_nj_out))
}
```


## Smooth Shuffle

```{r}
remin_shuffle_spec <- function(abun_in, ...){
  core <- remin_shuffle(abun_in, ...)
  abun_in + core$dnet
}
```

```{r}
trs_spec <- remin_shuffle_spec(test_abun_in, 0.99, 2, llb = .08) # .076, .102
trs_spec
```



```{r}
remin_smooth_shuffle <- function(abun_in, DFpct, Ipct = 0.9999, ...){
  # DFpct: Fractional mass retained between depths
  # Ipct: Fractional mass retained between iterations
  # ...: Passed to remin_shuffle
  IMirror <- 2  - Ipct
  
  abun_est = abun_in
  Fpct = DFpct # gets overwritten if we are iterating
  
  # If we are loossing flux, and we loose loose more flux than Ipct
  # Iterate remin shuffle only keeping ipct each time
  if(DFpct < Ipct){
    iters = floor(log(DFpct)/log(Ipct)) # why is this a ratio of log transformed values? 
    # I need to understand this before I can address the case where DFpct > IMirror
    iterFlux = Ipct^iters
    Fpct = 1-(iterFlux-DFpct)
    
    for (i in 1:iters){
      abun_est = remin_shuffle_spec(abun_in = abun_est, Ipct, ...)
    }
  }
  
  # If we are gaining flux, and we gain more than IMirror, iterate
  if(DFpct > IMirror){
    iters = floor(log(DFpct)/log(IMirror))
    iterFlux = IMirror^iters
    Fpct = 1 - (iterFlux - DFpct)
    
    for (i in 1:iters){
      abun_est = remin_shuffle_spec(abun_in = abun_est, IMirror, ...)
    }
  }
  
  # Deal with remainder. In the case where the loss is less than ipct, or greater than 2-ipct (Imirror), just do this part
  abun_est = remin_shuffle_spec(abun_in = abun_est, Fpct, ...)
  
  abun_est
}
```

```{r}
DFP = 0.8
trss0 <- remin_shuffle_spec(DFpct = DFP , abun_in = test_abun_in) # .076, .102
trss0
trss <- remin_smooth_shuffle(DFpct =  DFP, abun_in = test_abun_in) # .076, .102
trss
```

```{r}
tibble(test_abun_in, trss0, trss, lb_vec) %>% ggplot(aes(x = lb_vec)) + geom_point(aes(y = trss0)) + geom_point(aes(y = trss), shape = 2) + scale_y_log10() + scale_x_log10() + geom_point(aes(y = test_abun_in), shape = 1)
```



## Data
First, lets calculate Dfpct for each pair of depths, and lets also stagger spec



```{r}
dummy_function <- function(DFpct, spec, ...){
  DFpct * spec
}
dummy_function(0.8, test_abun_in)
```


```{r}
synthetic_remineralized <- synthetic_data %>% .[-1,] %>% mutate(pred_spec = map2(prev_spec, DFP, remin_smooth_shuffle))
synthetic_remineralized
```

Did it work?

```{r}
tibble(
actual = synthetic_remineralized[["spec_only"]][[4]],
predicted = synthetic_remineralized[["pred_spec"]][[4]],
previous = synthetic_remineralized[["prev_spec"]][[4]],
lb = lb_vec
) %>% gather(key = "var", value = "np", actual, predicted, previous) %>%
  ggplot(aes(x = lb, y = np, shape = var, col = var), ) + geom_point(size = 3, alpha = 0.75) + scale_x_log10() + scale_y_log10()
```
Frigging finally. Consistant everything I think.

Looks promising! Lets be more systematic about this next!

Ok. If I calculate flux assuming alpha = 2.3 and gamma = 1.3, I get this stronger than traditional arch effect.
I guess because flux attenuates faster than it should.

If I say that alpha + gamma = 0.63 as best fit flux, then I loose more big particles than small ones.
More resetting instnaces and now things look really strange.

But also my different flux profiles are no longer adding up.

# Analyzing remineralized data
Indicating which regions are actually loosing flux the whole time.
Lets first look at particle numbers, observed minus expected

```{r}
synthetic_remineralized %>% ggplot(aes(y = depth, x = DFP, col = DFP < 1)) + geom_point() + scale_y_reverse()
synthetic_remineralized %>% ggplot(aes(y = depth, x = Flux, col = DFP < 1)) + geom_point() + scale_y_reverse()
```

```{r}
synthetic_remineralized_proc <- synthetic_remineralized %>% mutate(TP = map_dbl(spec, sum), pred_TP = map_dbl(pred_spec, sum), dif_TP = TP-pred_TP) 
```

```{r}
dtpP <- synthetic_remineralized_proc %>% ggplot(aes(y = depth, x = dif_TP, col = DFP < 1)) + geom_point() + scale_y_reverse()
ptpP <- synthetic_remineralized_proc %>% ggplot(aes(y = depth, x = pred_TP, col = DFP < 1)) + geom_point() + scale_y_reverse() + scale_x_log10()
tpP <- synthetic_remineralized_proc %>% ggplot(aes(y = depth, x = TP, col = DFP < 1)) + geom_point() + scale_y_reverse() + scale_x_log10()
plot_grid(tpP, ptpP, dtpP)
```

Now that I've fixed this for positive values, we predict absurd numbers of particles much of the time.
Lets just look at decreasing parts.

```{r}
synthetic_remineralized_proc_decreasing <- synthetic_remineralized_proc %>% filter(DFP <= 1)
```

```{r}
dtpP <- synthetic_remineralized_proc_decreasing %>% ggplot(aes(y = depth, x = dif_TP, col = DFP < 1)) + geom_point() + scale_y_reverse()
ptpP <- synthetic_remineralized_proc_decreasing %>% ggplot(aes(y = depth, x = pred_TP, col = DFP < 1)) + geom_point() + scale_y_reverse() + scale_x_log10()
tpP <- synthetic_remineralized_proc_decreasing %>% ggplot(aes(y = depth, x = TP, col = DFP < 1)) + geom_point() + scale_y_reverse() + scale_x_log10()
fractpP <- synthetic_remineralized_proc_decreasing %>% ggplot(aes(y = depth, x = dif_TP/TP, col = DFP < 1)) + geom_point() + scale_y_reverse()
plot_grid(tpP, ptpP, dtpP, fractpP)
```

Oh dear. The model predicts profound particle number attenuation. Actually there isn't so everything actually just follows flux.

Maybe there's some better metric. Like fraction of flux on particles smaller than 53 micron or something.



This may have been a dead end. I need to reflect again.

```{r}
useRow <- which(synthetic_remineralized$DFP > 1)[1]

tibble(
actual = synthetic_remineralized[["spec_only"]][[useRow]],
predicted = synthetic_remineralized[["pred_spec"]][[useRow]],
previous = synthetic_remineralized[["prev_spec"]][[useRow]],
lb = lb_vec
) %>% gather(key = "var", value = "np", actual, predicted, previous) %>%
  ggplot(aes(x = lb, y = np, shape = var, col = var), ) + geom_point(size = 3, alpha = 0.75) + scale_x_log10() + scale_y_log10()
```
Ah. Somethings off here.

```{r}
test_abun_in
trss00 <- remin_smooth_shuffle(DFpct =  1, abun_in = test_abun_in) # .076, .102
trss00

trss00 <- remin_shuffle_spec(DFpct =  1.01, abun_in = test_abun_in) # .076, .102
trss00
```

# 13 April 2020
Hmm. Because of the power law nature of particles and size essentially "all" of the particles are created at each depth. Essentially particle number isn't a great proxie for disaggregation.

I have a couple of ideas.

One is to look at <53 miron vs >53 micron particles. One could ask:
How much biomass or flux moves from one side of that line to the other?

I'd also like to get at things like how particle production relates to flux attenuation. Like how much of flux attenuation is different with the small particles than if there weren't these small particles.

I could imagine calculating C_r everywhere and then just running a purely prism model propigating downward with that Cr. Seeing what flux is at some key threshold, and then comparing that to actual flux. I'd expect that essentially all of the attenuation woudl be through the production and removal of small particles.

In the back of my mind though, my assumptions about small particles are bugging me. What if they are more recalcitrant than big particles. Then all of the models sort of don't work, right?

# 20 April 2020
Some unnesting
```{r}
synthetic_remineralized_concise <- synthetic_remineralized %>%
  mutate(spec2 = map2(spec, prev_spec, ~tibble(.x, prev_np = .y))) %>%
  mutate(spec2 = map2(spec2, pred_spec, ~tibble(.x, pred_np = .y))) %>%
  select(-c(spec, prev_spec, pred_spec))

synthetic_remineralized_unnested <- synthetic_remineralized_concise %>% unnest(spec2)
```
I wonder what is up with these name reassignments?


Goal: Estimate attenuation of large and small particles. Compare that to total attenuation.
(Hopefully they sum to the same thing). 
Do this for both the model, and for the observed data.

Calculate flux transfer to small particles from disaggregation which should be 

ObservedSmallFlux - PredictedSmallFlux
Which should equal: PredictedBigFlux - ObservedSmallFlux


