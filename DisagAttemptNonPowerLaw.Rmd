---
title: "R Notebook"
output: html_notebook
---

The goal here is instead of assuming everything is a power law, I'll use a gam to interpolate each size bin.
Then we do the generally same analysis.

```{r}
library(tidyverse)
library(tidyselect)
pass <- function(x){x}
source("Remin_Library.R")
```

Data from elsewhere
```{r, message= FALSE}
options(readr.default_locale=readr::locale(tz="Mexico/General"))
source("UVP_2017_library.R")

ES01 <- read_csv("dataOut/binned_EachSize.csv") %>% filter(depth <= 1000)
DS01 <- read_csv("dataOut/binned_DepthSummary.csv") %>% filter(depth <= 1000)
twinS <- list(ES01, DS01)
```

```{r}
SimpleBins <- seq(from = 0, to = 1100, by = 50) # Modified the By

#debug(bin_depths)
binnedS <- bin_depths(twinS, bins = SimpleBins) %>% calc_psd
#binnedS <- bin_depths(twinS, bins = BianchiBins) %>% calc_psd

ESS <- binnedS[[1]]
DSS <- binnedS[[2]]
```

Group by bin, and fit a gam to each bin.
Then predict the particle numbers back out.
We're going to work in `nparticle` space

```{r}
gaminate00 <- ESS %>% filter(profile == "stn_043") %>%
  select(depth, TotalParticles, vol, lb) %>%
  group_by(lb) %>% 
  nest(data = c(TotalParticles, vol, depth)) %>%
  pass

# I think I want to family gamma this if it goes negative; or maybe bust poisson back out
mygam <- function(df){gam(TotalParticles~depth, offset = vol, data = df, family = "poisson")} 

# gaminate01 <- gaminate00 %>% 
#   mutate(bingam = map(data, mygam)) %>%
#   mutate(pred = map2(bingam, data, predict)) %>%
#   mutate(predDf = map2(data, pred, tibble)) %>%
#   mutate(predDf = map(predDf, ~rename(., pred = 3))) %>%
#   select(lb, predDf) %>%
#   unnest(cols = c(predDf)) %>%
#   pass
  
# gaminate01 %>% head
```

```{r}
# gaminate01 %>% filter(lb == 0.102) %>% ggplot(aes(y = depth, x = nparticles)) + geom_point() + scale_y_reverse() +
#   geom_point(aes(x = pred), shape = 1)
```

```{r}
gaminate00[[2]][[1]] -> test
#glm(TotalParticles~log(depth), offset = vol, data = test[,], family = "poisson")
# below fails
# gam(TotalParticles~s(depth), offset = vol, data = test[-1,], family = poisson)
```

```{r}
ESS %>% filter(profile == "stn_043") %>% group_by(lb) %>% ggplot(aes(x = TotalParticles, y = depth, col = log(lb), group = lb)) + scale_y_reverse() + geom_point() + scale_x_log10() + scale_color_viridis_c() + geom_path() + geom_vline(xintercept = 5)
```

What if any time we see < 5 Total particles, we fill in from the power law. Otherwise we use the observed number?

Ok. Loess seems to work.

Order of operations
(1) if total particles < 5 (or similar), use psd_big and int_big to estimate nparticles in that bin
(2) interpolate every size bin down to small sizes
(3) recalculate things that need to be recalculated (like flux and what not)
(4) calculate disaggregation

```{r}
particle_data <- ESS %>% select(-nparticles, -n_nparticles, -time, -binsize)
```

# Without Pre-Binning
# Back to pre binning
Forget pre-binning, that was mostly for stack overflow I think

Functionalize this
Uses psd to guess the number of particles when we see fewer than five of them
```{r}
get_np = function(icp, psd, bin, binsize){
  C_n = exp(icp)
  np = (C_n * bin ^ psd) * binsize
  np
}

#ES02 <- ES01 %>% # Without prebinning
ES02 <- ESS %>%   # With prebinning
  left_join(DS01 %>% select(profile, time, depth, big_psd, big_icp), by = c("profile", "time", "depth")) %>%
  mutate(nInterp = (exp(big_icp) * lb ^ big_psd) * binsize) %>%
  mutate(nparticles2 = if_else(TotalParticles < 5, nInterp, nparticles)) %>% 
  select(-big_icp, -big_psd)
```

Now, we want to make everything higher resolution.
I do this by using loess to get nparticles for every depth

```{r}

C_f = 10.51
ag = 0.78

ready_tibble <- tibble(depth = seq(from = 20, to = 1000, by = 5))

ES03 <- ES02 %>%
  filter(profile == "stn_043") %>%
  select(depth, lb, nparticles2) %>%
  group_by(lb) %>%
  #pull(depth) %>% unique %>%
  nest(data = c(depth, nparticles2)) %>%
  #mutate(loe = map(data, function(df) loess(nparticles2 ~ depth, data = df))) %>%
  #BOOKMARK
   # mutate(loe = map(data, function(df) gam(nparticles2 ~ s(depth), data = df, family = "Gamma"))) %>%
  # the below column is awesome, but fails when set to map2_df, and is not actually the right solution
  # # mutate(data2 = map2(loe, data, ~mutate(.y, pred = predict(.x, .y)))) %>%
  #mutate(data2 = map(loe, ~mutate(ready_tibble, np = predict(., ready_tibble, type = "response")))) %>%
  # select(-data, -loe) %>%
  # unnest(cols = data2) %>%
  # mutate(flux = np * C_f * lb ^ ag) %>%
  # ungroup() %>%
  pass
  
ES03
```

```{r}
# testDf <- ES03[["data"]][[24]]
# testDf
# 
# testMod <- gam(nparticles2 ~ s(depth), data = testDf, family = "Gamma")
# testMod
# testPred <- predict(testMod, data = testMod, type = "response")
# testPredDf <- bind_cols(testDf, pred = testPred)
# ggplot(aes(nparticles2, depth), data = testPredDf) + geom_point() + scale_y_reverse() + 
#   geom_path(aes(x = pred)) + scale_x_log10()
```


Ok, this isn't working with the close to zero values. Lets try this as a poisson GLM

Lets do one example first

```{r}
test02 <- ES02 %>% filter(profile == "stn_043", lb == 2.050)

test02

mod02 <- gam(TotalParticles ~ s(depth), offset = log(vol), family = poisson, data = test02)

predTest02 <- predict(mod02, data = test02, type = "response")
test02Df <- bind_cols(test02, pred = predTest02)
ggplot(data = test02Df, aes(x = pred, y = depth)) + geom_point() + scale_y_reverse() + scale_x_log10() + geom_point(aes(x = nparticles), shape = 1, size = 3)
```

# Re-attempt gam

```{r}

C_f = 10.51
ag = 0.78

ready_tibble <- tibble(depth = seq(from = 20, to = 1000, by = 10))
#ready_tibble <- tibble(depth = BianchiBins)

ES03 <- ES02 %>%
  filter(profile == "stn_043") %>%
  select(depth, lb, TotalParticles, vol) %>%
  group_by(lb) %>%
  #pull(depth) %>% unique %>%
  nest(data = c(depth, TotalParticles, vol)) %>%
  mutate(loe = map(data, function(df){gam(TotalParticles ~ s(depth, k = -1), offset = log(vol), family = "poisson", data = df)})) %>%
  # # mutate(data2 = map2(loe, data, ~mutate(.y, pred = predict(.x, .y)))) %>%
  mutate(data2 = map(loe, ~mutate(ready_tibble, np = predict(., ready_tibble, type = "response")))) %>%
  select(-data, -loe) %>%
  unnest(cols = data2) %>%
  mutate(flux = np * C_f * lb ^ ag) %>%
  ungroup() %>%
  pass
  
ES03
```

```{r}
ES03 %>% ggplot(aes(y = depth, x = np, col = lb, group = factor(lb))) + geom_path() + scale_y_reverse() + scale_color_viridis_c()
```

```{r}
ES03 %>% ggplot(aes(y = depth, x = flux, col = lb, group = factor(lb))) + geom_path() + scale_y_reverse() + scale_color_viridis_c()
```


Sanity check
```{r}

DS03 <- ES03 %>% group_by(depth) %>%
  nest(spec = c(lb, np, flux)) %>%
  ungroup() %>%
  mutate(Flux = map_dbl(spec, ~sum(.$flux))) %>%
  mutate(spec_only = map(spec, ~pull(., np)) )%>% 
  mutate(prev_spec = lag(spec_only)) %>%
  mutate(prev_Flux = lag(Flux)) %>%
  mutate(DF = prev_Flux - Flux, 
         DFP = Flux / prev_Flux) %>%
  pass

DS03
```

Now I think I want to make pred_spec here from prev_spec
I'll do this in three steps.

1) Use remin shuffle to get a new spec.
2) Now figure out what DFP needs to be to get the right flux
3) Recalculate spec with the corrected DFP

First a few tests
```{r}
test_abun_in <- DS03 %>% filter(depth == 50) %>% pull(spec) %>%.[[1]] %>% pull(np)
```

```{r}
test_abun_out <- remin_smooth_shuffle(test_abun_in, 0.9)
```

```{r}
tibble(lb = lb_vec, tin = test_abun_in, tout = test_abun_out) %>%
  pivot_longer(cols = c("tin", "tout")) %>%
  ggplot(aes(x = lb, y = value, color = name)) + geom_point() + scale_x_log10() + scale_y_log10() + cowplot::theme_cowplot()
```

```{r}
topt <- optFun(test_abun_in, 0.9)
topt
```
Misterious warning message. Don't see a clear answer online.

# Apply topt over everything

```{r}
DSA <- DS03 %>% slice(-1) %>%
  mutate(DFP_corrected = map2_dbl(prev_spec, DFP, optFun)) %>%
  mutate(pred_spec = map2(prev_spec, DFP_corrected, remin_smooth_shuffle))
DSA

```

I so don't remember what happens next.
I think that now, I calculate big flux, small flux, big predicted flux, small predicted flux
And then I calculate how much extra small predicted flux there is than you would expect from the remin model.
That devided by, something is the number I care about.
osps/(Prev_Flux_Big- Flux_Big)
osps =  Flux_Small - Pred_Flux_Small

But this should be equivalent to something more basic, right?
Flux_Small - Pred_Flux_Small = Pred_Flux_Big - Flux_Big = osps

so
(Pred_Flux_Big - Flux_Big)/ (Prev_Flux_Big - Flux_Big)

Calculate Flux, prev_Flux, and pred_Flux, big and small

```{r}
DSA01 <- DSA %>%
  select(depth, spec_only = spec_only, prev_spec, pred_spec) %>%
  pivot_longer(cols = c("spec_only", "prev_spec", "pred_spec"), values_to = "spec", names_to = "type") %>%
  mutate(fSpec = map(spec, function(spc){C_f_global * spc ^ ag_global})) %>%
  mutate(fSpec_Big = map(fSpec, ~.[lb_vec > 0.53]),
         fSpec_Small = map(fSpec, ~.[lb_vec <= 0.53])) %>%
  select(-spec) %>%
  pivot_longer(cols = contains("fSpec"), names_to = "SizeCat", values_to = "fSpec") %>%
  mutate(Flux = map_dbl(fSpec, sum)) %>%
  
  # mutate(Flux = map_dbl(fSpec, sum),
  #        Flux_Big = map_dbl(fSpec_Big, sum),
  #        Flux_Small = map_dbl(fSpec_Small, sum)) %>%
  # select(depth, type, contains("Flux")) %>%
  # pivot_longer(cols = contains("Flux"), names_to = "category") %>%
  mutate(type = recode(type, spec_only = "Here", prev_spec = "Prev",pred_spec = "Pred"),
         SizeCat = recode(SizeCat, fSpec = "All", fSpec_Big = "Big", fSpec_Small = "Small")) %>%
  select(-fSpec) %>%
  pivot_wider(names_from = c("type", "SizeCat"), values_from = "Flux") %>%
  # (Pred_Flux_Big - Flux_Big)/ (Prev_Flux_Big - Flux_Big)
  mutate(Disag_Big = (Pred_Big - Here_Big) / (Prev_Big - Here_Big)) %>%
  mutate(DFP = Here_All/Prev_All) %>%
  pass
  


DSA01

         #Flux_Check = Flux_Big + Flux_Small) # confirmed and removed
```

```{r}
DSA01 %>% ggplot(aes(y = depth, x = DFP)) + geom_point() + scale_y_reverse()
```

```{r}
DSA01 %>% ggplot(aes(y = depth, x = Here_All)) + geom_point() + scale_y_reverse()
```


```{r}
DSA01 %>% filter(DFP < 1) %>% ggplot(aes(y = depth, x = Disag_Big)) + scale_y_reverse() + geom_point()
```

Silly, but working.
Might explore pre binning again, to smooth things back out. Also not over chopping things up.